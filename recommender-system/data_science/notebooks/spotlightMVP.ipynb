{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SpotlightRec MVP\n",
    "Purpose: To construct a *sequencial* movie recommender that returns a recommendation based upon a list of liked films."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Spotlight specific tools\n",
    "from spotlight.cross_validation import user_based_train_test_split\n",
    "############################################################\n",
    "# Goal: remove the need for synthetic data\n",
    "#from spotlight.datasets.synthetic import generate_sequential\n",
    "# Actual data set\n",
    "from spotlight.datasets.movielens import get_movielens_dataset\n",
    "############################################################\n",
    "from spotlight.evaluation import sequence_mrr_score, sequence_precision_recall_score\n",
    "from spotlight.sequence.implicit import ImplicitSequenceModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generation of synthetic data using built in spotlight generator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#help(get_movielens_dataset)\n",
    "movielens = get_movielens_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Format of movielens dataset:\n",
    " - number of rows: 100000\n",
    " - columns include: user id | item id | rating | timestamp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construct train and test splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#help(user_based_train_test_split)\n",
    "train, test = user_based_train_test_split(movielens)\n",
    "\n",
    "train = train.to_sequence()\n",
    "test = test.to_sequence()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train model on user based review training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#help(ImplicitSequenceModel)\n",
    "model = ImplicitSequenceModel(n_iter=50,\n",
    "                              representation='cnn',\n",
    "                              loss='bpr')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: loss 0.3051327263767069\n",
      "Epoch 1: loss 0.21001177981044306\n",
      "Epoch 2: loss 0.20228308935960135\n",
      "Epoch 3: loss 0.20180538748249863\n",
      "Epoch 4: loss 0.19824618372050198\n",
      "Epoch 5: loss 0.19474790267872089\n",
      "Epoch 6: loss 0.19241349173314642\n",
      "Epoch 7: loss 0.19096127197597967\n",
      "Epoch 8: loss 0.18391118853381186\n",
      "Epoch 9: loss 0.16757668554782867\n",
      "Epoch 10: loss 0.15247104700767633\n",
      "Epoch 11: loss 0.14214567298238928\n",
      "Epoch 12: loss 0.13556200220729364\n",
      "Epoch 13: loss 0.12837566074096796\n",
      "Epoch 14: loss 0.12227996032346379\n",
      "Epoch 15: loss 0.11606480903697736\n",
      "Epoch 16: loss 0.112983236032905\n",
      "Epoch 17: loss 0.11190626499327747\n",
      "Epoch 18: loss 0.11051565995722105\n",
      "Epoch 19: loss 0.10940220590793726\n",
      "Epoch 20: loss 0.10865532764882753\n",
      "Epoch 21: loss 0.10883934931321577\n",
      "Epoch 22: loss 0.10666036470369859\n",
      "Epoch 23: loss 0.10645296447204822\n",
      "Epoch 24: loss 0.10542114259618701\n",
      "Epoch 25: loss 0.10586479193333423\n",
      "Epoch 26: loss 0.10272521841706651\n",
      "Epoch 27: loss 0.10313971972826755\n",
      "Epoch 28: loss 0.10296716473319313\n",
      "Epoch 29: loss 0.10132124771674474\n",
      "Epoch 30: loss 0.10139718403418858\n",
      "Epoch 31: loss 0.10134629521406058\n",
      "Epoch 32: loss 0.09909614717418497\n",
      "Epoch 33: loss 0.10278774910803998\n",
      "Epoch 34: loss 0.10016808342753035\n",
      "Epoch 35: loss 0.0989484100630789\n",
      "Epoch 36: loss 0.09896076177105759\n",
      "Epoch 37: loss 0.09880008548498154\n",
      "Epoch 38: loss 0.09871716278068947\n",
      "Epoch 39: loss 0.09742280931183786\n",
      "Epoch 40: loss 0.09673959265152614\n",
      "Epoch 41: loss 0.09642569720745087\n",
      "Epoch 42: loss 0.09597220700798613\n",
      "Epoch 43: loss 0.09578537647471283\n",
      "Epoch 44: loss 0.09477008653409554\n",
      "Epoch 45: loss 0.09566299955953252\n",
      "Epoch 46: loss 0.09483790239601424\n",
      "Epoch 47: loss 0.09388916442791621\n",
      "Epoch 48: loss 0.09354439806757552\n",
      "Epoch 49: loss 0.09440612386573445\n"
     ]
    }
   ],
   "source": [
    "#help(model.fit)\n",
    "model.fit(train, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00980392, 0.05      , 0.01010101, ..., 0.06666667, 0.04      ,\n",
       "       0.25      ])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequence_mrr_score(model, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#help(model.predict)\n",
    "predVals = model.predict(sequences=np.array([1,11,28]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "172"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(predVals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the returned prediction is simply the movieID. Based on the storage of the data (Sequential interaction database) we are lacking the ability to extract pertinant metadata about the predicted movie.\n",
    "\n",
    "Further work is being done on this problem."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.15 ('spotlightRec')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e439ed9c651532e781f85ad19c7ac41a9cb1deddb48b265bf0652d2bd6796a85"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
